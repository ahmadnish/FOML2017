\documentclass[11pt, a4paper]{scrartcl}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{url}
\usepackage{lmodern}        % Latin Modern family of fonts
\usepackage[T1]{fontenc}    % fontenc is oriented to output, that is, what fonts to use for printing characters. 
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\graphicspath{{../figs/}}
\usepackage{subcaption}
%\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{vmargin}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{csquotes}
\usepackage{bm}
\usepackage{placeins}
\usepackage{pdfpages}
\usepackage{blindtext}
\usepackage[list-final-separator={, and }]{siunitx}


%\setmarginsrb{3 cm}{2.5 cm}{3 cm}{2.5 cm}{1 cm}{1.5 cm}{1 cm}{1.5 cm}
\setmarginsrb{3 cm}{1.5 cm}{3 cm}{1.5 cm}{1 cm}{0.5 cm}{1 cm}{0.5 cm}
\title{Breast Cancer Wisconsin Data Set} % Title
\author{Jannik Lukas Kossen \& Ahmad Neishabouri}                               % Author
\date{\today}                                         % Date

\makeatletter
\let\thetitle\@title
\let\theauthor\@author
\let\thedate\@date
\makeatother

\pagestyle{fancy}
\fancyhf{}
\rhead{\theauthor}
\lhead{Fundamentals of Machine Learning}
\cfoot{\thepage}

% Fix BibTex error
\setcitestyle{numbers}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
    \centering
%    \vspace*{0.5 cm}
    \includegraphics[scale = 0.6]{hdlogo}\\[2.0 cm]  % University Logo
     \begin{flushleft}
     \large  \hspace{1cm} A report on 
	\end{flushleft}      
     \centering
    \rule{\linewidth}{0.2 mm} \\[0.4 cm]
    { \huge \bfseries \thetitle}\\
    \rule{\linewidth}{0.2 mm} \\[1.5 cm]
    
    \textsc{\LARGE Final Project}\\[0.5 cm]               % Course Code
    \textsc{\Large Heidelberg Collaboratory \\[0.5em] for Image Processing}\\[2.0 cm]  % University Name
    \thedate
   	\\[3em]
    \large
            \emph{Submitted to:}\\[1em]
            Prof. Dr. Ulrich Köthe \& Tutor: Lynton\\[1cm]
            \emph{Submitted by:} \\[1.5em]
            \begin{minipage}{0.4\textwidth}            
            	\begin{flushleft} 
					\large Jannik Lukas Kossen (JK) Student ID: 3495228 \\
					\small M.Sc. Physics, ungraded\\
      		  \end{flushleft}
      		 \end{minipage}
            \begin{minipage}{0.4\textwidth}            
            	\begin{flushright} 
    	    	    			\large Ahmad Neishabouri (AN) Student ID: 3436580 \\ 
 			        	\small M.Sc. Scientific Computing, graded
      		  \end{flushright}
      		 \end{minipage}\\[2 cm]


     
\end{titlepage}


\tableofcontents
\pagebreak


%TODO fix titlepage (names and everything)
%TODO proper cites of data set

\section{Introduction (JK)}
The following pages contain the report on our final project for the \emph{Fundamentals of Machine Learning} lecture given by Dr.\ Köthe in the last semester. As the lecture focussed on the \emph{fundamentals} of machine learning we see this project as an opportunity to explore and revise again some fundamental techniques of machine learning. Often, the application potential of these techniques on real-world tasks is rather small. However, they do constitute the foundations of more recent and better algorithms, which often embed or build upon these basic methods. 
The focus of this project shall therefore not be to \emph{solve} a task using fundamental machine learning, but rather to carefully study the different algorithms and compare their strengths and shortcomings in comparison with each other. To narrow the focus of the project a bit, we have chosen to only compare \emph{classification} methods.
We work with the \emph{Breast Cancer Wisconsin (Diagnostic) Data Set} \cite{street1993nuclear}, a binary classification task with the aim to identify whether a person does or does not have breast cancer based on a set of hand-crafted features. The data set should work very well for benchmarking algorithms, providing a nice balance between being well-behaved, i.e. solvable, and possessing enough depth and character to allow for some comparison between the algorithms, i.e. not being trivial to solve.

\paragraph{Structure.} The structure of this report is as follows. \Cref{sec:theo} discusses the theoretical foundations of each of the compared methods. In \cref{sec:expe}, unsupervised exploration techniques are employed to gain some understanding of the shape of our data set. The classification methods are applied and benchmarked. \Cref{sec:discu} then discusses these results in relation to peculiarities of each method and the data set.

As demanded by the faculty of Mathematics and Computer Science, each section or paragraph heading also contains a shorthand symbol, \emph{(JK)} or \emph{(AN)}, to indiciate whether a certain section was written by Jannik Kossen or Ahmad Neishabouri.

%\begin{figure}
%	\centering
%	\includegraphics[width=0.7\textwidth]{hdlogo}
%	\caption{A figure.}
%	\label{fig:class-incremental}
%\end{figure}

\section{Theoretical Background}
\label{sec:theo}

\paragraph{Preliminary Remarks (JK).}
Note that, unless marked otherwise, the theoretical derivation and description attempt follow the notation of the lecture \cite{kothe2018foml} for clarity.
This includes the convention to describe the data as $X = \left\{ X_1, X_2, \ddots, X_N \right\}$, where $N$ is the number of instances and each $X_i$ is a $D$-dimensional feature (row) vector. The feature $j$  of instance $i$ is therefore given by $X_{ij}$.  Index $i$ is solely used to index along the $N$-dimensional instance axis, whereas $j$ solely indexes the $D$-dimensional feature axis.




\section{Experiments}
\label{sec:expe}

\subsection{Data Set -- Breast Cancer Wisconsin (JK)}
Before we 


\section{Discussion}
\label{sec:discu}

\section{Summary}

\section{Appendix}


\bibliographystyle{ieeetr}
\bibliography{bibliography}

\end{document}